services:
  ml_host_backend_test:
    build:
      context: ./services/ml_host_backend
      dockerfile: Dockerfile
      target: test
    container_name: ml_host_backend_test

  ml_train_hub_test:
    container_name: ml_train_hub_test
    image: ml_train_hub:test #  ðŸ‘ˆ This sets the name and tag
    build:
      context: ./services/ml_train_hub
      dockerfile: Dockerfile
      target: test

  ml_host_backend_prod:
    build:
      context: ./services/ml_host_backend
      dockerfile: Dockerfile
      target: prod
    depends_on:
      - ml_host_backend_test # Ensure tests pass before starting prod
    ports:
    - "8080:8080"
    container_name: ml_host_backend_prod
    command: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]

  ml_train_hub_prod:
    container_name: ml_train_hub_prod
    image: ml_train_hub:prod #  ðŸ‘ˆ This sets the name and tag, consider version numbers from ENV variable
    build:
      context: ./services/ml_train_hub
      dockerfile: Dockerfile
      target: prod
    depends_on:
      - ml_train_hub_test # Ensure tests pass before starting prod
    volumes:
      - ./mlruns:/home/services/ml_train_hub/mlruns # adapt the local folder for the prod server, where the mlruns are stored
      - ./file_exchange:/home/services/ml_train_hub/file_exchange # adapt the local folder for the prod server, how files shall be exchanged
    ports:
    - "8081-8082:8081-8082" # MLflow server (prod:8081) and FastAPI server (prod:8082)
    command: ["./entrypoint.sh"]
